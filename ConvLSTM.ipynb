{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Ukako_2jTxg"
   },
   "source": [
    "## Load Module & Fix Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "biskGLBcjceC"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import glob\n",
    "import re\n",
    "from netCDF4 import Dataset\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ug-0lg5Djnzi"
   },
   "outputs": [],
   "source": [
    "# Fix Seed\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AspIkcox2GX7"
   },
   "source": [
    "## Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kKlupGAcEzEQ"
   },
   "outputs": [],
   "source": [
    "# Feature Data\n",
    "climate_file = np.load(\"./preprocessed/feature_final444.npz\")\n",
    "climate_data = climate_file['data']\n",
    "climate_data = np.delete(climate_data, 4, axis=1) # Many missing values -> Need to Delete (from original dataset)\n",
    "\n",
    "# Target Data\n",
    "sic_file = np.load(\"./preprocessed/NSIDC_seaice_con_199501_202412.npz\")\n",
    "sic_data = sic_file['sic']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ha_wj08Pl8-o"
   },
   "source": [
    "## Custom Sea Ice Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GnA8OV5CEXnJ"
   },
   "outputs": [],
   "source": [
    "class SeaIceDataset(Dataset):\n",
    "    def __init__(self, climate_array, sic_array, window_length, start_idx, end_idx):\n",
    "        self.climate = climate_array\n",
    "        self.sic = sic_array\n",
    "        self.L = window_length                 # Sliding window size\n",
    "\n",
    "        self.start = start_idx + self.L\n",
    "        self.end = end_idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.end - self.start + 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        t = self.start + idx\n",
    "\n",
    "        # Feature Sequence\n",
    "        seq_climate = self.climate[t-self.L : t]   # (L, 10, 428, 300)\n",
    "        # Target\n",
    "        target_sic = self.sic[t] # (428, 300)\n",
    "        target_sic = np.expand_dims(target_sic, axis=0) # (1, 428, 300)\n",
    "\n",
    "        seq_climate = torch.from_numpy(seq_climate).float()\n",
    "        target_sic = torch.from_numpy(target_sic).float()\n",
    "\n",
    "        return seq_climate, target_sic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qyHrgOxEEvj6"
   },
   "outputs": [],
   "source": [
    "train_dataset = SeaIceDataset(climate_array=climate_data, sic_array=sic_data, window_length=12, idx_start=0, idx_end=287)\n",
    "val_dataset = SeaIceDataset(climate_array=climate_data, sic_array=sic_data, window_length=12, idx_start=288, idx_end=323)\n",
    "test_dataset = SeaIceDataset(climate_array=climate_data, sic_array=sic_data, window_length=12, idx_start=324, idx_end=359)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=1, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, num_workers=1, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, num_workers=1, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "efiyGIHhmm08"
   },
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rcAK_LkLFTSu"
   },
   "outputs": [],
   "source": [
    "# Code by GPT to reproduce the paper\n",
    "class ConvLSTMCell(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size, bias=True):\n",
    "        super(ConvLSTMCell, self).__init__()\n",
    "        self.input_dim  = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = kernel_size[0] // 2, kernel_size[1] // 2\n",
    "        self.bias = bias\n",
    "\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels=self.input_dim + self.hidden_dim,\n",
    "            out_channels=4 * self.hidden_dim,\n",
    "            kernel_size=self.kernel_size,\n",
    "            padding=self.padding,\n",
    "            bias=self.bias\n",
    "        )\n",
    "\n",
    "    def forward(self, x, h_prev, c_prev):\n",
    "        # 1) 입력 x와 이전 은닉 h_prev를 채널 차원으로 concatenate\n",
    "        combined = torch.cat([x, h_prev], dim=1)  # (batch, input_dim+hidden_dim, H, W)\n",
    "\n",
    "        # 2) 한 번의 Conv 연산으로 4가지 게이트에 필요한 연산량을 모두 계산\n",
    "        #    출력 채널 수는 4*hidden_dim\n",
    "        combined_conv = self.conv(combined)  # (batch, 4*hidden_dim, H, W)\n",
    "\n",
    "        # 3) 채널을 4개 그룹으로 분리\n",
    "        cc_i, cc_f, cc_g, cc_o = torch.split(\n",
    "            combined_conv, self.hidden_dim, dim=1\n",
    "        )\n",
    "        # cc_i: 입력 게이트 전 단계 연산 결과\n",
    "        # cc_f: 망각 게이트 전 단계 연산 결과\n",
    "        # cc_g: 신규 셀 상태 제안치 전 단계 연산 결과\n",
    "        # cc_o: 출력 게이트 전 단계 연산 결과\n",
    "\n",
    "        # 4) 활성화 함수 적용\n",
    "        i = torch.sigmoid(cc_i)             # 입력 게이트\n",
    "        f = torch.sigmoid(cc_f)             # 망각 게이트\n",
    "        g = torch.tanh(cc_g)                # 셀 상태 후보\n",
    "        o = torch.sigmoid(cc_o)             # 출력 게이트\n",
    "\n",
    "        # 5) 새로운 셀 상태 계산\n",
    "        c_next = f * c_prev + i * g\n",
    "\n",
    "        # 6) 새로운 은닉 상태 계산\n",
    "        h_next = o * torch.tanh(c_next)\n",
    "\n",
    "        return h_next, c_next\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XEWI8Z5dIX5c"
   },
   "outputs": [],
   "source": [
    "class ConvLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, kernel_size, num_layers, batch_first=False, bias=True, return_all_layers=False):\n",
    "        super(ConvLSTM, self).__init__()\n",
    "        self.input_dim  = input_dim\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.kernel_size = kernel_size\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_first = batch_first\n",
    "        self.bias = bias\n",
    "        self.return_all_layers = return_all_layers\n",
    "\n",
    "        layers = []\n",
    "        for i in range(self.num_layers):\n",
    "            cur_input_dim = self.input_dim if i == 0 else self.hidden_dims[i-1]\n",
    "            layers.append(\n",
    "                ConvLSTMCell(\n",
    "                    input_dim=cur_input_dim,\n",
    "                    hidden_dim=self.hidden_dims[i],\n",
    "                    kernel_size=self.kernel_size[i],\n",
    "                    bias=self.bias\n",
    "                )\n",
    "            )\n",
    "        self.cell_list = nn.ModuleList(layers)\n",
    "\n",
    "    def forward(self, x, hidden_states=None):\n",
    "        if not self.batch_first:\n",
    "            x = x.permute(1, 0, 2, 3, 4) # (L, B, C, H, W) → (B, L, C, H, W)\n",
    "\n",
    "        batch_size, seq_len, _, height, width = x.size()\n",
    "\n",
    "        # 초기 은닉 상태가 주어지지 않았다면, 0으로 초기화\n",
    "        if hidden_states is None:\n",
    "            hidden_states = []\n",
    "            for i in range(self.num_layers):\n",
    "                h = torch.zeros(batch_size, self.hidden_dims[i], height, width, device=x.device)\n",
    "                c = torch.zeros(batch_size, self.hidden_dims[i], height, width, device=x.device)\n",
    "                hidden_states.append((h, c))\n",
    "\n",
    "        layer_output_list = []\n",
    "        last_state_list   = []\n",
    "\n",
    "        cur_input = x  # 첫 레이어의 입력은 전체 시퀀스\n",
    "\n",
    "        # 레이어마다 순차 삽입\n",
    "        for layer_idx in range(self.num_layers):\n",
    "            h, c = hidden_states[layer_idx]\n",
    "            output_inner = []  # 이 레이어 내에서 각 시점별 은닉 상태를 담을 리스트\n",
    "\n",
    "            # 각 시점(time-step)마다 ConvLSTMCell을 호출\n",
    "            for t in range(seq_len):\n",
    "                # t번째 time-step 입력 (batch, C, H, W)\n",
    "                x_t = cur_input[:, t, :, :, :]\n",
    "                h, c = self.cell_list[layer_idx](x_t, h, c)\n",
    "                output_inner.append(h)  # 은닉 상태만 저장 (c는 내부에서 관리)\n",
    "\n",
    "            # output_inner: 길이 L인 은닉 상태 텐서 리스트. (각각 (B, hidden_dim, H, W))\n",
    "            # 이를 (B, L, hidden_dim, H, W) 텐서로 합침\n",
    "            layer_output = torch.stack(output_inner, dim=1)\n",
    "            layer_output_list.append(layer_output)\n",
    "\n",
    "            # 해당 레이어의 마지막 은닉 상태와 셀 상태를 저장\n",
    "            last_state_list.append((h, c))\n",
    "\n",
    "            # 다음 레이어 입력: 현재 레이어의 전 시퀀스 은닉 출력\n",
    "            cur_input = layer_output\n",
    "\n",
    "        # return:\n",
    "        #  - layer_output_list: [레이어1 전체 시퀀스 출력, 레이어2 전체 시퀀스 출력, ...]\n",
    "        #  - last_state_list:   [(h_n1, c_n1), (h_n2, c_n2), ...]\n",
    "        if not self.return_all_layers:\n",
    "            layer_output_list = layer_output_list[-1:]  # 마지막 레이어만 남김\n",
    "            last_state_list   = last_state_list[-1:]\n",
    "\n",
    "        return layer_output_list, last_state_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aHmusOYzIcL7"
   },
   "outputs": [],
   "source": [
    "class SeaIceConvLSTM(nn.Module):\n",
    "    def __init__(self, input_channels=10, hidden_channels=64, kernel_size=(3, 3), lstm_layers=1, height=428, width=300, bias=True):\n",
    "        super(SeaIceConvLSTM, self).__init__()\n",
    "\n",
    "        # 1) ConvLSTM 모듈\n",
    "        self.convlstm = ConvLSTM(\n",
    "            input_dim=input_channels,\n",
    "            hidden_dims=[hidden_channels] * lstm_layers,\n",
    "            kernel_size=[kernel_size] * lstm_layers,\n",
    "            num_layers=lstm_layers,\n",
    "            batch_first=True,     # 입력 x는 (B, L, C, H, W)\n",
    "            bias=bias,\n",
    "            return_all_layers=False\n",
    "        )\n",
    "\n",
    "        # 2) 출력 Head: 마지막 은닉 상태 → SIC 예측\n",
    "        # ConvLSTM의 마지막 레이어 은닉은 (B, 64, H, W)\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=hidden_channels,\n",
    "            out_channels=32,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "            bias=bias\n",
    "        )\n",
    "        self.relu  = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=32,\n",
    "            out_channels=1,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "            bias=bias\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 1) ConvLSTM에 시퀀스를 넣으면,\n",
    "        #    layer_output_list = [(B, L, hidden_channels, H, W)]\n",
    "        #    last_state_list   = [(h_n, c_n)] 에서\n",
    "        #    h_n: (B, hidden_channels, H, W)\n",
    "        layer_output_list, last_state_list = self.convlstm(x)\n",
    "\n",
    "        # ConvLSTM 마지막 레이어의 마지막 시점 은닉 상태 (h_n)\n",
    "        h_n, c_n = last_state_list[0]     # 튜플 (h_n, c_n)\n",
    "        # h_n shape: (B, hidden_channels, 428, 300)\n",
    "\n",
    "        # 2) 후처리: conv → ReLU → conv\n",
    "        x = self.conv1(h_n)    # (B, 32, H, W)\n",
    "        x = self.relu(x)\n",
    "        out = self.conv2(x)   # (B, 1, H, W)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WpY1FtiUIjOH"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_epochs = 50\n",
    "model = SeaIceConvLSTM(\n",
    "    input_channels=10,\n",
    "    hidden_channels=64,\n",
    "    kernel_size=(3,3),\n",
    "    lstm_layers=1,\n",
    "    height=428,\n",
    "    width=300,\n",
    "    bias=True\n",
    ").to(device)\n",
    "\n",
    "# Loss & Optimizer & Learning rate Scheduler\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.5)\n",
    "\n",
    "best_val_loss = float('inf')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wVW0lPRo1031"
   },
   "source": [
    "## Train & Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ny-p5UEJItLh"
   },
   "outputs": [],
   "source": [
    "for epoch in range(1, num_epochs+1):\n",
    "    # Train\n",
    "    model.train()\n",
    "    total_train_loss = 0.0\n",
    "    for seq_climate, target_sic in train_loader:\n",
    "        seq_climate = seq_climate.to(device) # (B, L, 11, 428, 300)\n",
    "        target_sic  = target_sic.to(device) # (B, 1, 428, 300)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        pred_sic = model(seq_climate)            # (B, 1, 428, 300)\n",
    "        loss = criterion(pred_sic, target_sic)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_train_loss += loss.item() * seq_climate.size(0)\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_loader.dataset)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    total_val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for seq_climate, target_sic in val_loader:\n",
    "            seq_climate = seq_climate.to(device)\n",
    "            target_sic = target_sic.to(device)\n",
    "\n",
    "            pred_sic = model(seq_climate)\n",
    "            loss = criterion(pred_sic, target_sic)\n",
    "            total_val_loss += loss.item() * seq_climate.size(0)\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_loader.dataset)\n",
    "\n",
    "    scheduler.step(avg_val_loss)\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': avg_val_loss\n",
    "        }, 'best_seaice_convlstm.pth')\n",
    "\n",
    "    print(f\"[Epoch {epoch}/{num_epochs}] Train Loss = {avg_train_loss:.6f}  |  Val Loss = {avg_val_loss:.6f}  |  LR = {optimizer.param_groups[0]['lr']:.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BVXvC2vZcofA"
   },
   "source": [
    "## Test & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fdgMWmK-chQH"
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "test_preds = []\n",
    "test_trues = []\n",
    "test_losses = 0.0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for seq_climate, target_sic in test_loader:\n",
    "        seq_climate = seq_climate.to(device)\n",
    "        target_sic = target_sic.to(device)\n",
    "\n",
    "        pred_sic = model(seq_climate)\n",
    "        loss = criterion(pred_sic, target_sic)\n",
    "        test_losses += loss.item() * seq_climate.size(0)\n",
    "\n",
    "        test_preds.append(pred_sic.cpu().numpy())\n",
    "        test_trues.append(target_sic.cpu().numpy())\n",
    "\n",
    "avg_test_loss = test_losses / len(test_loader.dataset)\n",
    "print(f\"Average Test Loss = {avg_test_loss:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VnyQQmOjc75w"
   },
   "outputs": [],
   "source": [
    "def plot_sic_comparison(pred, true, idx, save_path=None):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "    im0 = axes[0].imshow(true[0], vmin=0, vmax=1, cmap='Blues')\n",
    "    axes[0].set_title(f\"Ground Truth (Idx {idx})\")\n",
    "    axes[0].axis('off')\n",
    "    plt.colorbar(im0, ax=axes[0], fraction=0.046, pad=0.04)\n",
    "\n",
    "    im1 = axes[1].imshow(pred[0], vmin=0, vmax=1, cmap='Blues')\n",
    "    axes[1].set_title(f\"Prediction (Idx {idx})\")\n",
    "    axes[1].axis('off')\n",
    "    plt.colorbar(im1, ax=axes[1], fraction=0.046, pad=0.04)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if save_path is not None:\n",
    "        plt.savefig(save_path, dpi=150)\n",
    "    else:\n",
    "        plt.show()\n",
    "    plt.close()\n",
    "\n",
    "# 예시: 테스트 10번째 샘플을 시각화\n",
    "sample_idx = 10\n",
    "plot_sic_comparison(pred=test_preds[sample_idx], true=test_trues[sample_idx], idx=sample_idx)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyORY2Hze12Laa81NteGJVkC",
   "mount_file_id": "18smDHoVXc7IsmqBauivgiEfMT6v4PeyW",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
